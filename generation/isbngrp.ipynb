{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     dctx \u001b[38;5;241m=\u001b[39m zstandard\u001b[38;5;241m.\u001b[39mZstdDecompressor()\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dctx\u001b[38;5;241m.\u001b[39mstream_reader(compressed_file) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[0;32m---> 13\u001b[0m         \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# byt = reader.read()\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m# record = orjson.loads(byt)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#         except Exception as e:\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#             print(f\"Error reading line: {e}\")\u001b[39;00m\n",
      "\u001b[0;31mUnsupportedOperation\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import zstandard\n",
    "import json\n",
    "import sys\n",
    "import orjson\n",
    "\n",
    "\n",
    "file_path = \"../data/annas_archive_meta__aacid__isbngrp_records__20240920T194930Z--20240920T194930Z.jsonl.seekable.zst\"\n",
    "\n",
    "\n",
    "# with open(file_path, 'rb') as compressed_file:\n",
    "#     dctx = zstandard.ZstdDecompressor()\n",
    "#     with dctx.stream_reader(compressed_file) as reader:\n",
    "#         reader.readline()\n",
    "        # byt = reader.read()\n",
    "        # record = orjson.loads(byt)\n",
    "\n",
    "\n",
    "# # Replace with the actual file name (you can dynamically set the filename if needed)\n",
    "# timestamp = \"20230123T123456Z\"  # Replace with the actual timestamp used\n",
    "# filename = f\"annas_archive_meta__aacid__isbngrp_records__{timestamp}--{timestamp}.jsonl\"\n",
    "\n",
    "# # Open the JSONL file and process it line by line\n",
    "# with open(filename, 'rb') as file:\n",
    "#     for line in file:\n",
    "#         try:\n",
    "#             # Deserialize the JSON line into a Python object\n",
    "            \n",
    "            \n",
    "#             # Example: Extracting specific fields from the record\n",
    "#             aacid = record.get(\"aacid\")\n",
    "#             metadata = record.get(\"metadata\", {})\n",
    "#             record_id = metadata.get(\"id\")\n",
    "#             record_content = metadata.get(\"record\", {})\n",
    "            \n",
    "#             print(f\"AACID: {aacid}\")\n",
    "#             print(f\"Record ID: {record_id}\")\n",
    "#             print(f\"Record Content: {record_content}\")\n",
    "#             print(\"-\" * 50)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error reading line: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zstandard\n",
    "import orjson\n",
    "import csv\n",
    "\n",
    "# Input and output file names\n",
    "output_csv_filename = \"output.csv\"\n",
    "\n",
    "# Open the compressed file and CSV file\n",
    "with open(file_path, 'rb') as compressed_file, open(output_csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    # Initialize the Zstandard decompressor\n",
    "    dctx = zstandard.ZstdDecompressor()\n",
    "    with dctx.stream_reader(compressed_file) as reader:\n",
    "        # Initialize a CSV writer\n",
    "        csv_writer = csv.writer(csv_file, quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "        \n",
    "        # Write the header row to the CSV\n",
    "        csv_writer.writerow([\"registrant_name\", \"agency_name\", \"country_name\", \"isbns\"])\n",
    "        \n",
    "        # Create a buffer to read lines incrementally\n",
    "        buffer = b\"\"\n",
    "        i = 0\n",
    "        while True:\n",
    "            i += 1\n",
    "\n",
    "        # for _ in range(100 000):\n",
    "            chunk = reader.read(8192)  # Read chunks from the decompressed stream\n",
    "            if not chunk:\n",
    "                break  # End of file\n",
    "\n",
    "            buffer += chunk\n",
    "            # Process lines in the buffer\n",
    "            while b'\\n' in buffer:\n",
    "                line, buffer = buffer.split(b'\\n', 1)  # Split into a line and the remaining buffer\n",
    "                try:\n",
    "                    # Parse the JSON object from the line\n",
    "                    record = orjson.loads(line)\n",
    "                    \n",
    "                    # Extract relevant fields\n",
    "                    # aacid = record.get(\"aacid\")\n",
    "                    metadata = record.get(\"metadata\", {})\n",
    "                    # record_id = metadata.get(\"id\")\n",
    "                    record_content = metadata.get(\"record\", {})\n",
    "                    \n",
    "                    # Extract record details\n",
    "                    registrant_name = record_content.get(\"registrant_name\")\n",
    "                    agency_name = record_content.get(\"agency_name\")\n",
    "                    country_name = record_content.get(\"country_name\")\n",
    "                    \n",
    "                    # Flatten the ISBNs for the CSV (comma-separated list of ISBNs)\n",
    "                    isbns = \"; \".join(\n",
    "                        f\"{isbn['isbn']} ({isbn['isbn_type']})\" for isbn in record_content.get(\"isbns\", [])\n",
    "                    )\n",
    "                    \n",
    "                    # Write the row to the CSV\n",
    "                    csv_writer.writerow([registrant_name, agency_name, country_name, isbns])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing line: {e}\")\n",
    "        \n",
    "        # Handle any remaining buffer (last line without newline)\n",
    "        if buffer.strip():\n",
    "            try:\n",
    "                record = orjson.loads(buffer)\n",
    "                aacid = record.get(\"aacid\")\n",
    "                metadata = record.get(\"metadata\", {})\n",
    "                record_id = metadata.get(\"id\")\n",
    "                record_content = metadata.get(\"record\", {})\n",
    "                \n",
    "                registrant_name = record_content.get(\"registrant_name\")\n",
    "                agency_name = record_content.get(\"agency_name\")\n",
    "                country_name = record_content.get(\"country_name\")\n",
    "                isbns = \"; \".join(\n",
    "                    f\"{isbn['isbn']} ({isbn['isbn_type']})\" for isbn in record_content.get(\"isbns\", [])\n",
    "                )\n",
    "                \n",
    "                csv_writer.writerow([aacid, record_id, registrant_name, agency_name, country_name, isbns])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing last line: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
